{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "tSjJuvKTyjbo",
   "metadata": {
    "id": "tSjJuvKTyjbo"
   },
   "source": [
    "# Setup our vulnerability data science lab environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "QVjRIuM97Wv4",
   "metadata": {
    "id": "QVjRIuM97Wv4"
   },
   "source": [
    "First we'll import all the libraries we need. A couple of them need installed first. JQ is a pythonic implementation of jq; a tool for querying json really fast. When looking at 25 years of vulnerabilities it is enormously useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "Ac3vLew6N8Ik",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ac3vLew6N8Ik",
    "outputId": "d82755ff-c8b6-42ce-cb0a-faa60dec1195"
   },
   "outputs": [],
   "source": [
    "#!pip install requests\n",
    "#!pip install hurst\n",
    "#!pip install jq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aPf4YVs47Hr3",
   "metadata": {
    "id": "aPf4YVs47Hr3"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import gzip as gz\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import json\n",
    "import datetime\n",
    "import tqdm\n",
    "import os\n",
    "import jq\n",
    "import json\n",
    "import itertools\n",
    "import numpy as np\n",
    "import datetime\n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "from sklearn.metrics import mean_absolute_percentage_error as mape\n",
    "import re\n",
    "from pandas.plotting import autocorrelation_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HaE6Q136LzU2",
   "metadata": {
    "id": "HaE6Q136LzU2"
   },
   "source": [
    "The folders where we will store the data as a gzip, and as a json need to be created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "DMJg36yV7NU6",
   "metadata": {
    "id": "DMJg36yV7NU6"
   },
   "outputs": [],
   "source": [
    "file_exists = os.path.exists('CVE-NVD')\n",
    "if not file_exists:\n",
    "  os.mkdir('CVE-NVD')\n",
    "  os.mkdir('CVE-NVD/GZIP')\n",
    "  os.mkdir('CVE-NVD/JSON')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lqd6pK-_y3V2",
   "metadata": {
    "id": "lqd6pK-_y3V2"
   },
   "source": [
    "Let's also setup some other folders for MITRE's advance views of CVE data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "FgvWYE3HVV1J",
   "metadata": {
    "id": "FgvWYE3HVV1J"
   },
   "outputs": [],
   "source": [
    "file_exists = os.path.exists('CVE-MITRE')\n",
    "if not file_exists:\n",
    "  os.mkdir('CVE-MITRE')\n",
    "  os.mkdir('CVE-MITRE/CSV')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Peg0lTsMy_g0",
   "metadata": {
    "id": "Peg0lTsMy_g0"
   },
   "source": [
    "# convert datetime obj to string\n",
    "str_current_datetime = str(current_datetime)\n",
    "  \n",
    "# create a file object along with extension\n",
    "file_name = str_current_datetime+\".txt\"# Download the CVE data from NVD and MITRE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d_nZqbNCS4vE",
   "metadata": {
    "id": "d_nZqbNCS4vE"
   },
   "source": [
    "Now we'll download the NVD data for every year since 1999. Don't worry it's faster than you think.  \n",
    "PROTIP: The progress bar comes for free from the tqdm package. Just wrap a for loop in tqdm.tqdm() it and you get a progress bar for free.\n",
    "Now after this tutorial if you keep this notebook, you'll always be able to fetch all this CVE data easily. Handy for many more things than just forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "mhHNC2b87SNZ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mhHNC2b87SNZ",
    "outputId": "13c13822-b8ef-45a4-a463-8c0acd1e4837"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:27<00:00,  1.08s/it]\n"
     ]
    }
   ],
   "source": [
    "now = datetime.datetime.now()\n",
    "#PROTIP wrap an iterative loop in python with tqdm.tqdm() and you get a progress bar\n",
    "for i in tqdm.tqdm(range(1999,now.year+1)):\n",
    "    url = 'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-'+str(i)+'.json.gz'\n",
    "    req = requests.get(url, stream=True)\n",
    "    with open('CVE-NVD/GZIP/nvdcve-1.1-'+str(i)+'.json.gz', 'wb') as f:\n",
    "        for chunk in req.iter_content(chunk_size=1024):\n",
    "            if chunk:\n",
    "                f.write(chunk)\n",
    "                f.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "btOEgkvVTGpV",
   "metadata": {
    "id": "btOEgkvVTGpV"
   },
   "source": [
    "Here we need to decompress all those gzip files so we can work with the data as json files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "CsOo8_Nv7VSF",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CsOo8_Nv7VSF",
    "outputId": "11f45d40-41a0-47ce-852b-9cf79b0f2dd5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:04<00:00,  5.53it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm.tqdm(range(1999,now.year+1)):\n",
    "    with gz.open('CVE-NVD/GZIP/nvdcve-1.1-'+str(i)+'.json.gz', 'rb') as f_in:\n",
    "        with open('CVE-NVD/JSON/nvdcve-1.1-'+str(i)+'.json', 'wb')  as f_out:\n",
    "            shutil.copyfileobj(f_in, f_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1obZacReVYcn",
   "metadata": {
    "id": "1obZacReVYcn"
   },
   "source": [
    "Now we need to download the MITRE version of CVEs too, which gives us a different kind of information that is useful later. Specifically, it gives us a view of CVEs that didn't make the cut, and some other timestamps we can use to show when CVEs where submitted, as opposed to published. That can help us calculate the rate of publication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8zEaEjpIWO5T",
   "metadata": {
    "id": "8zEaEjpIWO5T"
   },
   "outputs": [],
   "source": [
    "url = 'https://cve.mitre.org/data/downloads/allitems.csv'\n",
    "req = requests.get(url, stream=True)\n",
    "now = datetime.datetime.now()\n",
    "with open('CVE-MITRE/CSV/allitems_current.csv', 'wb') as f:\n",
    "    for chunk in req.iter_content(chunk_size=1024):\n",
    "        if chunk:\n",
    "            f.write(chunk)\n",
    "            f.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MsUxquJB2Am4",
   "metadata": {
    "id": "MsUxquJB2Am4"
   },
   "source": [
    "# Convert the data to panda dataframes and csv files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nHa3etpy2YJT",
   "metadata": {
    "id": "nHa3etpy2YJT"
   },
   "source": [
    "Here we start to use JQ to make queiries specific to CVE json structure. We pull out the CVE-ID, the published date, the assigner, and the CVSSv2 base score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "Bkc7OhffsWxZ",
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bkc7OhffsWxZ",
    "outputId": "9cb6697c-a620-4723-98f1-f35c079e93d1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [08:03<00:00, 21.98s/it]\n"
     ]
    }
   ],
   "source": [
    "cve_id_query = jq.compile(\".CVE_Items[].cve.CVE_data_meta.ID\")\n",
    "cve_publication_query = jq.compile(\".CVE_Items[].publishedDate\")\n",
    "cve_assigner_query = jq.compile(\".CVE_Items[].cve.CVE_data_meta.ASSIGNER\")\n",
    "cve_description_query = jq.compile(\".CVE_Items[].cve.description.description_data[].value\")\n",
    "cvss_v2_score_query = jq.compile(\".CVE_Items[].impact.baseMetricV2.cvssV2.baseScore\")\n",
    "cvss_v2_exploitability_score_query = jq.compile(\".CVE_Items[].impact.baseMetricV2.cvssV2.exploitabilityScore\")\n",
    "cvss_v2_vector_query = jq.compile(\".CVE_Items[].impact.baseMetricV2.cvssV2.vectorString\")\n",
    "cvss_v3_score_query = jq.compile(\".CVE_Items[].impact.baseMetricV3.cvssV3.baseScore\")\n",
    "cvss_v3_exploitability_score_query = jq.compile(\".CVE_Items[].impact.baseMetricV3.cvssV3.exploitabilityScore\")\n",
    "cvss_v3_vector_query = jq.compile(\".CVE_Items[].impact.baseMetricV3.cvssV3.vectorString\")\n",
    "cwe_query = jq.compile(\".CVE_Items[].cve.problemtype.problemtype_data[].description[].value\")\n",
    "now = datetime.datetime.now()\n",
    "collector = []\n",
    "for i in tqdm.tqdm(range(2002, now.year+1)):\n",
    "    with open('CVE-NVD/JSON/nvdcve-1.1-'+str(i)+'.json') as json_file:\n",
    "        data = json.load(json_file)\n",
    "        pubs = cve_publication_query.input(data).text()\n",
    "        pubs = pubs.split('\\n')\n",
    "        pubs = [pd.to_datetime(ts.strip('\"'), yearfirst=True, infer_datetime_format=True) for ts in pubs]\n",
    "        index= pd.Series(pubs,name='Publication')\n",
    "        cves = cve_id_query.input(data).text()\n",
    "        cves = cves.split('\\n')\n",
    "        cves = [cve.strip('\"') for cve in cves]\n",
    "        assigners = cve_assigner_query.input(data).text()\n",
    "        assigners = assigners.split('\\n')\n",
    "        description = cve_description_query.input(data).text()\n",
    "        description = description.split('\\n')\n",
    "        cvss_v2 = cvss_v2_score_query.input(data).text()\n",
    "        cvss_v2 = cvss_v2.split('\\n')\n",
    "        cvss_v2_vector = cvss_v2_vector_query.input(data).text()\n",
    "        cvss_v2_vector = cvss_v2_vector.split('\\n')\n",
    "        cvss_v2_exploitability = cvss_v2_exploitability_score_query.input(data).text()\n",
    "        cvss_v2_exploitability = cvss_v2_exploitability.split('\\n')\n",
    "        cvss_v3 = cvss_v3_score_query.input(data).text()\n",
    "        cvss_v3 = cvss_v3.split('\\n')\n",
    "        cvss_v3_vector = cvss_v3_vector_query.input(data).text()\n",
    "        cvss_v3_vector = cvss_v3_vector.split('\\n')\n",
    "        cvss_v3_exploitability = cvss_v3_exploitability_score_query.input(data).text()\n",
    "        cvss_v3_exploitability = cvss_v3_exploitability.split('\\n')\n",
    "        cwe = cwe_query.input(data).text()\n",
    "        cwe = cwe.split('\\n')\n",
    "        s1 = pd.Series(cves,name='ID')\n",
    "        s2 = pd.Series(assigners,name='ASSIGNER')\n",
    "        s3 = pd.Series(description,name='DESCRIPTION')\n",
    "        ones = [1]*len(cves)\n",
    "        s4 = pd.Series(ones,name='Count')\n",
    "        s5 = pd.Series(cvss_v2,name='v2 CVSS')\n",
    "        s6 = pd.Series(cvss_v2_vector,name='v2 Vector')\n",
    "        s7 = pd.Series(cvss_v2_exploitability,name='v2 Exploitability Score')\n",
    "        s8 = pd.Series(cvss_v3,name='v3 CVSS')\n",
    "        s9 = pd.Series(cvss_v3_vector,name='v3 Vector')\n",
    "        s10 = pd.Series(cvss_v3_exploitability,name='v3 Expoitability Score')\n",
    "        s11 = pd.Series(cwe,name='CWE')\n",
    "        vulns = pd.concat([index,s1,s2,s3,s4,s5,s6,s7,s8,s9,s10,s11], axis=1)\n",
    "        vulns = vulns.set_index('Publication')\n",
    "        collector.append(vulns)\n",
    "        json_file.close()\n",
    "all_items = pd.concat(collector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sgzeBTCf2w1R",
   "metadata": {
    "id": "sgzeBTCf2w1R"
   },
   "source": [
    "Save all the data we just filtered to a CSV file, for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5a634b0",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "d5a634b0"
   },
   "outputs": [],
   "source": [
    "all_items.sort_index()\n",
    "all_items.to_csv('NVD-Vulnerability-Volumes.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fEoLX86f2727",
   "metadata": {
    "id": "fEoLX86f2727"
   },
   "source": [
    "If you want to read that file in the future, without fetching all the data again, just uncoment the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d2a5b2e",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "2d2a5b2e",
    "outputId": "955e5b09-f4da-4c1c-cdca-92930b5fac07"
   },
   "outputs": [],
   "source": [
    "all_items = pd.read_csv('NVD-Vulnerability-Volumes.csv',index_col=['Publication'],parse_dates=['Publication'],infer_datetime_format=True)\n",
    "all_items = all_items.sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c2b711-94af-4a73-aa8d-775214ee4b1e",
   "metadata": {},
   "source": [
    "# NVD API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdf6c73-f5af-49bc-8f91-e10877c5b1b4",
   "metadata": {},
   "source": [
    "As an alternative to the above you can use the NVD API, which is the method NVD requests you use and also the one that will be supported in the future. [Info on deprecation timeline here](https://nvd.nist.gov/general/news/api-20-announcements) and [info on the use of the API here](https://nvd.nist.gov/developers/vulnerabilities). API-keys are free adn more information about getting them are available at the urls mentioned. API keys are not necessary, but will increase the rate you are able to pull the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "8b07cace-dd9f-45f4-8916-c0dc66641e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_exists = os.path.exists('./CVE-NVD-API')\n",
    "if not file_exists:\n",
    "    os.mkdir('./CVE-NVD-API')\n",
    "    os.mkdir('./CVE-NVD-API/JSON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "77208840-9fda-48a1-bc64-a2ed7d8dd83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#API_key = \"<YOUR API KEY>\"\n",
    "headers = {'User-Agent': 'I am scraping this data for research purposes. Please do not block. Contact me at <your email>'}\n",
    "#headers['apiKey'] = API_key # With an API key\n",
    "base_url = \"https://services.nvd.nist.gov/rest/json/cves/2.0/\"\n",
    "sess = requests.Session()\n",
    "sess.headers.update(headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "6716b6d2-520b-421f-a167-22e2be94073a",
   "metadata": {},
   "outputs": [],
   "source": [
    "info_resp = sess.get(base_url, params = {\"resultsPerPage\":1000, \"startIndex\":0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "79bb3d66-75da-40bd-b88c-593067dcffa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total results 226582\n"
     ]
    }
   ],
   "source": [
    "if info_resp.status_code == 200:\n",
    "    d = json.loads(info_resp.text)\n",
    "    total_results = d.get('totalResults', None)\n",
    "    if total_results is None:\n",
    "        print(\"Problem fetching total results\")\n",
    "    else:\n",
    "        print(\"Total results \" + str(total_results))\n",
    "else:\n",
    "    print(\"API failed to respond. Status code: \" + str(info_resp.status_code))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b99e56-86b3-414d-8c39-8f65d6971689",
   "metadata": {},
   "source": [
    "THe [API best practices](https://nvd.nist.gov/developers/start-here) indicate that there is a 5 requests in a rolling 30second window for non-API key requests and 50 in a rolling 30 second window for API validated. We'll then want to sleep between each request a little bit more than 6 seconds (or .6 if you have an API key), just to ensure we are not overloading them. At this rate it will take more than 35 minutes. If you have an API key, it'll be a little more than 3.5 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "b44db69a-cb70-40b7-90f0-a4530cbdd995",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "731461be-3dd7-4b98-b790-e20b302854ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "rpp = 1000 # Technically this is 5000, but may be timed out at lower levels\n",
    "start_is = range(0, total_results, rpp)\n",
    "params = {\"resultsPerPage\":rpp}\n",
    "time_delay = 6.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "8390964c-02b7-4d64-a2fd-2e893f17780b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already scraped index 0\n",
      "Already scraped index 1000\n",
      "Already scraped index 2000\n",
      "Already scraped index 3000\n",
      "Already scraped index 4000\n",
      "Already scraped index 5000\n",
      "Already scraped index 6000\n",
      "Already scraped index 7000\n",
      "Already scraped index 8000\n",
      "Already scraped index 9000\n",
      "Already scraped index 10000\n",
      "Already scraped index 11000\n",
      "Already scraped index 12000\n",
      "Already scraped index 13000\n",
      "Already scraped index 14000\n",
      "Already scraped index 15000\n",
      "Already scraped index 16000\n",
      "Already scraped index 17000\n",
      "Already scraped index 18000\n",
      "Already scraped index 19000\n",
      "Already scraped index 20000\n",
      "Already scraped index 21000\n",
      "Already scraped index 22000\n",
      "Already scraped index 23000\n",
      "Already scraped index 24000\n",
      "Already scraped index 25000\n",
      "Already scraped index 26000\n",
      "Already scraped index 27000\n",
      "Already scraped index 28000\n",
      "Already scraped index 29000\n",
      "Already scraped index 30000\n",
      "Already scraped index 31000\n",
      "Already scraped index 32000\n",
      "Already scraped index 33000\n",
      "Already scraped index 34000\n",
      "Already scraped index 35000\n",
      "Already scraped index 36000\n",
      "Already scraped index 37000\n",
      "Already scraped index 38000\n",
      "Already scraped index 39000\n",
      "Already scraped index 40000\n",
      "Already scraped index 41000\n",
      "Already scraped index 42000\n",
      "Already scraped index 43000\n",
      "Already scraped index 44000\n",
      "Already scraped index 45000\n",
      "Already scraped index 46000\n",
      "Already scraped index 47000\n",
      "Already scraped index 48000\n",
      "Already scraped index 49000\n",
      "Already scraped index 50000\n",
      "Already scraped index 51000\n",
      "Already scraped index 52000\n",
      "Already scraped index 53000\n",
      "Already scraped index 54000\n",
      "Already scraped index 55000\n",
      "Already scraped index 56000\n",
      "Already scraped index 57000\n",
      "Already scraped index 58000\n",
      "Already scraped index 59000\n",
      "Already scraped index 60000\n",
      "Already scraped index 61000\n",
      "Already scraped index 62000\n",
      "Already scraped index 63000\n",
      "Already scraped index 64000\n",
      "Already scraped index 65000\n",
      "Already scraped index 66000\n",
      "Already scraped index 67000\n",
      "Already scraped index 68000\n",
      "Already scraped index 69000\n",
      "Already scraped index 70000\n",
      "Already scraped index 71000\n",
      "Already scraped index 72000\n",
      "Already scraped index 73000\n",
      "Already scraped index 74000\n",
      "Already scraped index 75000\n",
      "Already scraped index 76000\n",
      "Already scraped index 77000\n",
      "Already scraped index 78000\n",
      "Already scraped index 79000\n",
      "Already scraped index 80000\n",
      "Already scraped index 81000\n",
      "Already scraped index 82000\n",
      "Already scraped index 83000\n",
      "Already scraped index 84000\n",
      "Already scraped index 85000\n",
      "Already scraped index 86000\n",
      "Already scraped index 87000\n",
      "Already scraped index 88000\n",
      "Already scraped index 89000\n",
      "Already scraped index 90000\n",
      "Already scraped index 91000\n",
      "Already scraped index 92000\n",
      "Already scraped index 93000\n",
      "Already scraped index 94000\n",
      "Already scraped index 95000\n",
      "Already scraped index 96000\n",
      "Already scraped index 97000\n",
      "Already scraped index 98000\n",
      "Already scraped index 99000\n",
      "Already scraped index 100000\n",
      "Already scraped index 101000\n",
      "Already scraped index 102000\n",
      "Already scraped index 103000\n",
      "Already scraped index 104000\n",
      "Already scraped index 105000\n",
      "Already scraped index 106000\n",
      "Already scraped index 107000\n",
      "Already scraped index 108000\n",
      "Already scraped index 109000\n",
      "Already scraped index 110000\n",
      "Already scraped index 111000\n",
      "Already scraped index 112000\n",
      "Already scraped index 113000\n",
      "Already scraped index 114000\n",
      "Already scraped index 115000\n",
      "Already scraped index 116000\n",
      "Already scraped index 117000\n",
      "Already scraped index 118000\n",
      "Already scraped index 119000\n",
      "Already scraped index 120000\n",
      "Already scraped index 121000\n",
      "Already scraped index 122000\n",
      "Already scraped index 123000\n",
      "Already scraped index 124000\n",
      "Already scraped index 125000\n",
      "Already scraped index 126000\n",
      "Already scraped index 127000\n",
      "Already scraped index 128000\n",
      "Already scraped index 129000\n",
      "Already scraped index 130000\n",
      "Already scraped index 131000\n",
      "Already scraped index 132000\n",
      "Already scraped index 133000\n",
      "Already scraped index 134000\n",
      "Already scraped index 135000\n",
      "Already scraped index 136000\n",
      "Already scraped index 137000\n",
      "Already scraped index 138000\n",
      "Already scraped index 139000\n",
      "Already scraped index 140000\n",
      "Already scraped index 141000\n",
      "Already scraped index 142000\n",
      "Already scraped index 143000\n",
      "Already scraped index 144000\n",
      "Already scraped index 145000\n",
      "Already scraped index 146000\n",
      "Already scraped index 147000\n",
      "Already scraped index 148000\n",
      "Already scraped index 149000\n",
      "Already scraped index 150000\n",
      "Already scraped index 151000\n",
      "Already scraped index 152000\n",
      "Already scraped index 153000\n",
      "Already scraped index 154000\n",
      "Already scraped index 155000\n",
      "Already scraped index 156000\n",
      "Already scraped index 157000\n",
      "Already scraped index 158000\n",
      "Already scraped index 159000\n",
      "Already scraped index 160000\n",
      "Already scraped index 161000\n",
      "Already scraped index 162000\n",
      "Already scraped index 163000\n",
      "Already scraped index 164000\n",
      "Already scraped index 165000\n",
      "Already scraped index 166000\n",
      "Already scraped index 167000\n",
      "Already scraped index 168000\n",
      "Already scraped index 169000\n",
      "Already scraped index 170000\n",
      "Already scraped index 171000\n",
      "Already scraped index 172000\n",
      "Already scraped index 173000\n",
      "Already scraped index 174000\n",
      "Already scraped index 175000\n",
      "Already scraped index 176000\n",
      "Already scraped index 177000\n",
      "Already scraped index 178000\n",
      "Already scraped index 179000\n",
      "Already scraped index 180000\n",
      "Already scraped index 181000\n",
      "Already scraped index 182000\n",
      "Already scraped index 183000\n",
      "Already scraped index 184000\n",
      "Already scraped index 185000\n",
      "Already scraped index 186000\n",
      "Already scraped index 187000\n",
      "Already scraped index 188000\n",
      "Already scraped index 189000\n",
      "Already scraped index 190000\n",
      "Already scraped index 191000\n",
      "Already scraped index 192000\n",
      "Already scraped index 193000\n",
      "Already scraped index 194000\n",
      "Already scraped index 195000\n",
      "Already scraped index 196000\n",
      "Already scraped index 197000\n",
      "Already scraped index 198000\n",
      "Already scraped index 199000\n",
      "Already scraped index 200000\n",
      "Already scraped index 201000\n",
      "Already scraped index 202000\n",
      "Already scraped index 203000\n",
      "Already scraped index 204000\n",
      "Already scraped index 205000\n",
      "Already scraped index 206000\n",
      "Already scraped index 207000\n",
      "Already scraped index 208000\n",
      "Already scraped index 209000\n",
      "Already scraped index 210000\n",
      "Already scraped index 211000\n",
      "Already scraped index 212000\n",
      "Already scraped index 213000\n",
      "Already scraped index 214000\n",
      "Already scraped index 215000\n",
      "Already scraped index 216000\n",
      "Already scraped index 217000\n",
      "Already scraped index 218000\n",
      "Already scraped index 219000\n",
      "Already scraped index 220000\n",
      "Already scraped index 221000\n",
      "Already scraped index 222000\n",
      "Already scraped index 223000\n",
      "Already scraped index 224000\n",
      "Already scraped index 225000\n",
      "Already scraped index 226000\n"
     ]
    }
   ],
   "source": [
    "for i, start_i in enumerate(start_is):\n",
    "    fname = './CVE-NVD-API/JSON/' + str(start_i) + \"_\" + str(rpp) + '.json'\n",
    "    if os.path.isfile(fname):\n",
    "        print(\"Already scraped index \" + str(start_i))\n",
    "        continue\n",
    "    else:\n",
    "        params['startIndex'] = start_i\n",
    "        try:\n",
    "            resp = sess.get(base_url, params=params)\n",
    "        except requests.Timeout:\n",
    "            print(\"Request timed out for \" + str(start_i))\n",
    "            continue\n",
    "        if resp.status_code != 200:\n",
    "            print(\"Failed to get \" + str(start_i) + \" with status code \" + str(resp.status_code))\n",
    "            continue\n",
    "        try:\n",
    "            d = json.loads(resp.text)\n",
    "        except:\n",
    "            print(\"failed to load json \" + str(start_i))\n",
    "            continue\n",
    "        with open(fname, 'w') as f:\n",
    "            json.dump(d, f)\n",
    "        print(\"Successfully saved \" + str(start_i))\n",
    "        loop_delay = random.uniform(time_delay,time_delay*1.1)\n",
    "        print(\"Waiting \" + str(loop_delay) + \" seconds\")\n",
    "        time.sleep(loop_delay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726ce0f2-a026-4c32-8630-42eb88888f0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
